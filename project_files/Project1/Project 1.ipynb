{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Improving a Model's Performance Without Changing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this project, your task is to improve the performance of a pre-implemented model without changing the model itself. You will work on cleaning and pre-processing the dataset, extracting meaningful features, performing hyperparameter tuning, and implementing cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills You'll Develop\n",
    "- Data pre-processing (clean-up, normalization, data categorisation, handling missing values)\n",
    "- Feature engineering and feature impact analysis\n",
    "- Hyperparameter tuning\n",
    "- Cross-validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Throughout the notebook, you'll find empty sections with instructions on what to do. Follow these instructions to enhance the model's performance step by step.\n",
    "\n",
    "Before starting, run every cell to see what the starting performance is, and fill it in on the table below and then rerun all cells for every task you complete to track your progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Summary Table\n",
    " | Step                                               | Accuracy Score |\n",
    " |----------------------------------------------------|--------------------------|\n",
    " | Initial Model                                      |                          |\n",
    " | After Handling Missing Values                      |                          |\n",
    " | After Removing Duplicates                          |                          |\n",
    " | After Feature Engineering                          |                          |\n",
    " | After Normalization                                |                          |\n",
    " | After Hyperparameter Tuning                        |                          |\n",
    " | After Hyperparameter Tuning with Cross Validation  |                          |\n",
    " | Final Model Evaluation                             |                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We are using a property value dataset, which is messy and requires cleaning and preparation before it can be used effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('property_data.csv', keep_default_na=False) # Load the dataset\n",
    "Seed = 42 # Random seed to used to ensure reproducibility\n",
    "best_params = None # Placeholder for best parameters\n",
    "\n",
    "print(df.head()) # Display the first 5 rows of the dataset\n",
    "\n",
    "x = df.drop(columns=['SalePrice'])\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Analyse and visualise the dataset\n",
    "Before you can get to cleaning and pre-processing the dataset, you will want to analyse and visualise the dataset to get a better understanding on what needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset, look at the kind of data within the dataset, what changes might need to be made\n",
    "# visualize the dataset if needbe to understand the data better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Pre-processing\n",
    "The dataset is rather messy, and alot of the data is in an unoptimal format, this section you will clean up the dataset and perform any preprocessing needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Handle Missing Values\n",
    "The dataset contains some missing values that need to be addressed. You can either remove rows with missing values or fill them with appropriate values (e.g., mean, median).\n",
    "\n",
    "At the moment all missing data and NaNs are replaced with zeros but this is far from the best solution. Replace the missign data and NaNs with more meaningful values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with a better method\n",
    "for column in x.columns:\n",
    "    x[column] = x[column].replace('', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Remove Duplicates\n",
    "Identify and remove any duplicate entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Handle NaNs\n",
    "AI models can't handle NaNs very well and need to be handled.\n",
    "\n",
    "At the moment all NaNs in numeric features are replaced with zeros, but this is far from the best solution. Replace the NaNs with more meaningful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a better solution to handle NA values\n",
    "for column in x.columns:\n",
    "    temp = x[column][x[column] != 'NA'] # Gets all values except 'NA'\n",
    "    try: # Tries to convert the values to numeric\n",
    "        temp = pd.to_numeric(temp) # Tries to convert the values to numeric\n",
    "        x[column] = x[column].replace('NA', 0) # Replaces 'NA' with 0\n",
    "        x[column] = pd.to_numeric(x[column]) # Converts the column to numeric\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Handle Categotical Data\n",
    " Some features may contain categorical data and need to be converted to a formtat of which the model can handle. \n",
    " \n",
    " At the moment they are handled with one-hot encoding, but there are other ways to handle categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the code from the previous cell is repeated here which converts any nessessary columns to numeric\n",
    "# so that the the one-hot encoded used below doesnt categorise numeric data. Its here incase you removed\n",
    "# changed the code from the previous cell (which is encouraged)\n",
    "for column in df.columns:\n",
    "    try:\n",
    "        df[column] = pd.to_numeric(df[column])\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "x = pd.get_dummies(x) # replace this with a better solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Normalize the Data\n",
    "Sometimes data contain large numbers can negatively impact the model and so normalizing or standizing the features as needed as improve performance.\n",
    "\n",
    "Determine which columns can and should be normalized and then normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.head()) # Run this to see the first 5 rows of the dataset after any changes made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    " Not all features contribute to model performance. There are two ways to handle this: Feature Selection or Feature Transformation.\n",
    "\n",
    " Note: Whilst you will try both solutions, you will only use one of them in the end so pick whichever one you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Feature Selection\n",
    "Analyze the dataset and decide which features are most relevant for predicting property values using a feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Feature Tranformation\n",
    "Instead of selecting the best features and removing the worst, combine similar features together to create new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the Dataset\n",
    "Splitting the data into training and testing sets to evaluate the model's performance. \n",
    "\n",
    "No Changes need to be made here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=Seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning\n",
    "A model has many setting that can be fine tuned to further improve a model and whilst these can be tweaked manually, due to the amount of settings and the respective large number of combinations; sometimes its best to automate the hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Simple Tuning\n",
    "Use a method such as Grid search or Randomized Search to find the best hyperparameters. It is common practice to further split the training data (maybe a 80/20 split) into training and validation sets so the hyperparameter tuning is performed on a seperate dataset then what it is trained on to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Name the final parameters 'best_params'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Cross-validation Tuning\n",
    "An alteratnive form of tuning is called cross-validation. Perform K-fold cross validation tuning inplace of the method you attempted in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Name the final parameters 'best_params'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Below is an implementation of the Decision tree model. Its designed to run on the initial dataset loaded in and any improvements you make so no changes on your end should need to be made on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "if best_params is None: # If best_params is not provided, use the default model\n",
    "    model = DecisionTreeRegressor(random_state=Seed) \n",
    "else:\n",
    "    model = DecisionTreeRegressor(random_state=Seed, **best_params)\n",
    "\n",
    "model.fit(X_train, y_train) # train the model\n",
    "\n",
    "# evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "\n",
    "Display = True # Set to True to display the final tree, this could take a while to load\n",
    "\n",
    "if Display:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    tree.plot_tree(model, filled=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Summarize the steps you took to improve the model's performance and reflect on which changes had the most impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR SUMMARY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extention Task: Implement a differnt model and compare\n",
    "\n",
    "Now you've improved the performance of the Decision Tree model, try implementing a different model to see if you can get better results. You can use any model you like, such as Random Forest, SVM, or Gradient Boosting. Make sure to follow the same steps as above to clean and preprocess the data, tune the hyperparameters, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR NEW MODEL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "If you have any feedback about this project at all, feel free to tell us using this form: https://forms.gle/oCWaTdUbmwpjgLxi8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
